# -*- coding: utf-8 -*-
"""4050-Final-Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aXKkZk8fM0JVaERjcIk96XDoDMMuKCcK

# Loading in the Data
"""

import kagglehub
import pandas as pd
import os

# Download latest version
path = kagglehub.dataset_download("abdmoiz/walmart-stock-data-2025")

print("Path to dataset files:", path)

# Assuming there's only one CSV file in the folder (or you're targeting a specific one)
for file in os.listdir(path):
    if file.endswith(".csv"):
        csv_path = os.path.join(path, file)
        break

# Load the dataset into a pandas DataFrame
df = pd.read_csv(csv_path)

# Display the first few rows
print(df.head())

"""# Dataset Information"""

# Show general info about the dataset (column types, non-null counts, etc.)
print("========================================================= Dataset Info ===========================================================")
df.info()

# Show a basic statistical summary of numerical columns
print("\n================================================== Summary Statistics ==========================================================")
print(df.describe())

# Show a description of all columns including non-numeric ones
print("\n=============================================== Full Description (First 5 Rows) ================================================")
print(df.head())

"""Addressing Empty Values"""

# Check for null values in each column
print("=== Null Values in Each Column ===")
print(df.isnull().sum())

# Optional: see percentage of missing data per column
print("\n=== Percentage of Missing Data ===")
print((df.isnull().mean() * 100).round(2))

"""#Changing the Data Types"""

#I want to convert the column 'date' from an object into a date type
df['date'] = pd.to_datetime(df['date'], utc=True).dt.tz_convert(None)

print(df.dtypes)

"""# Feature Engineering"""

#add in classification if stock did go up or down
df['Increase_or_Decrease'] = df.apply(
    lambda row: 'Increase' if row['close'] > row['open'] else 'Decrease',
    axis=1
)

#add column to see amount price changed
df['Price_Change'] = df['close'] - df['open']

#add column to see percent of change per day
df['pct_change'] = ((df['close'] - df['open']) / df['open']) * 100

#add volatility column to determine market stability
df['volatility'] = (df['high'] - df['low']) / df['open']

#extracting datetime features (day, month, and year separately
df['year'] = df['date'].dt.year
df['month'] = df['date'].dt.month
df['day'] = df['date'].dt.day

df.head()

"""# Understanding the Spread"""

import matplotlib.pyplot as plt
import seaborn as sns

# Set plotting style
sns.set(style="whitegrid")
numeric_cols = ['open', 'high', 'low', 'close', 'adj_close', 'volume',
                'Price_Change', 'pct_change', 'volatility']

# Plot histograms
plt.figure(figsize=(16, 12))
for i, col in enumerate(numeric_cols, 1):
    plt.subplot(3, 3, i)
    sns.histplot(df[col], bins=50, kde=True)
    plt.title(f'Distribution of {col}')
plt.tight_layout()
plt.show()

"""## Addressing Close, High, Low, Open, and Adj_Close Columns"""

# due to open, close, high, low, and adj_close all being bimodal and skewed right, I am using a log transform
import numpy as np

# Columns to transform
price_cols = ['open', 'high', 'low', 'close', 'adj_close']

# Apply log transform to reduce skew
for col in price_cols:
    df[f'log_{col}'] = np.log(df[col])

#now I will standardize the 5 columns I just applied log transformation on
from sklearn.preprocessing import StandardScaler

log_cols = [f'log_{col}' for col in ['open', 'high', 'low', 'close', 'adj_close']]
scaler = StandardScaler()
df_scaled = pd.DataFrame(scaler.fit_transform(df[log_cols]), columns=[f'z_{col}' for col in log_cols])
df = pd.concat([df, df_scaled], axis=1)

#this is good because it shows the 3 phases of walmart stock:
# 1)new, low-price stock from 2008-early 2010's
# 2)medium-priced stock from mid 2010's
# 3)high-prices stock in from 2020-2025
import matplotlib.pyplot as plt
import seaborn as sns

# Visualize original vs log-transformed distributions
price_cols = ['open', 'high', 'low', 'close', 'adj_close']
plt.figure(figsize=(18, 10))

for i, col in enumerate(price_cols, 1):
    plt.subplot(2, 5, i)
    sns.histplot(df[col], bins=50, kde=True)
    plt.title(f'Original: {col}')

    plt.subplot(2, 5, i + 5)
    sns.histplot(df[f'log_{col}'], bins=50, kde=True, color='orange')
    plt.title(f'Log Transformed: {col}')

plt.tight_layout()
plt.show()

"""## Addressing the Volumn Column"""

#Now I take care of the volume column
df['log_volume'] = np.log(df['volume'])

# Visualize original vs log-transformed distributions
plt.figure(figsize=(12, 5))

# Original volume
plt.subplot(1, 2, 1)
sns.histplot(df['volume'], bins=50, kde=True)
plt.title('Original Volume Distribution')

# Log-transformed volume
plt.subplot(1, 2, 2)
sns.histplot(df['log_volume'], bins=50, kde=True, color='orange')
plt.title('Log-Transformed Volume Distribution')

plt.tight_layout()
plt.show()

scaler = StandardScaler()
df['z_log_volume'] = scaler.fit_transform(df[['log_volume']])

"""## Addressing Volatility Column"""

df['log_volatility'] = np.log(df['volatility'])
df['volatility'] = df['volatility'].replace(0, 1e-6)  # replace zero with a tiny value
df['log_volatility'] = np.log(df['volatility'])

scaler = StandardScaler()
df['z_log_volatility'] = scaler.fit_transform(df[['log_volatility']])

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(15, 5))

# Original
plt.subplot(1, 3, 1)
sns.histplot(df['volatility'], bins=50, kde=True)
plt.title('Original Volatility')

# Log-Transformed
plt.subplot(1, 3, 2)
sns.histplot(df['log_volatility'], bins=50, kde=True, color='orange')
plt.title('Log Volatility')

# Standardized
plt.subplot(1, 3, 3)
sns.histplot(df['z_log_volatility'], bins=50, kde=True, color='green')
plt.title('Standardized Log Volatility')

plt.tight_layout()
plt.show()

"""# Handling Outliers"""

plt.figure(figsize=(16, 12))
for i, col in enumerate(numeric_cols, 1):
    plt.subplot(3, 3, i)
    sns.boxplot(y=df[col])
    plt.title(f'Boxplot of {col}')
plt.tight_layout()
plt.show()

"""#Did not use below code as this filtered out ~20% of the code"""

# def filter_outliers_iqr(df, columns):
#     df_filtered = df.copy()
#     for col in columns:
#         Q1 = df[col].quantile(0.25)
#         Q3 = df[col].quantile(0.75)
#         IQR = Q3 - Q1
#         lower = Q1 - 1.5 * IQR
#         upper = Q3 + 1.5 * IQR
#         df_filtered = df_filtered[(df_filtered[col] >= lower) & (df_filtered[col] <= upper)]
#     return df_filtered

# # Columns to check for outliers
# columns_to_filter = ['open', 'close', 'high', 'low', 'adj_close', 'volume', 'volatility', 'Price_Change', 'pct_change']

# # Create a new DataFrame with outliers removed
# df_filtered = filter_outliers_iqr(df, columns_to_filter)

# #comparing the filtered dataset to the original dataset
# print("Original shape:", df.shape)
# print("Filtered shape:", df_filtered.shape)
# percentage_removed = ((df.shape[0] - df_filtered.shape[0]) / df.shape[0]) * 100
# print("Percentage of data removed:", round(percentage_removed, 2), "%")

def add_outlier_flag(df, col):
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    df[f'{col}_outlier'] = ((df[col] < lower) | (df[col] > upper)).astype(int)
    return df
columns_to_flag = [
    'open', 'close', 'high', 'low', 'adj_close',
    'volume', 'volatility', 'Price_Change', 'pct_change'
]

# Add outlier flag columns for each
for col in columns_to_flag:
    df = add_outlier_flag(df, col)
for col in columns_to_flag:
    outlier_count = df[f'{col}_outlier'].sum()
    print(f'{col}: {outlier_count} outliers ({round(100 * outlier_count / len(df), 2)}%)')

"""## Automating this all with a Pipeline"""

class WalmartStockPipeline:
    def __init__(self, df):
        self.df = df.copy()
        self.scaler_price = StandardScaler()
        self.scaler_volume = StandardScaler()
        self.scaler_volatility = StandardScaler()

    def convert_date(self):
        self.df['date'] = pd.to_datetime(self.df['date'], utc=True).dt.tz_convert(None)

    def create_classification_and_features(self):
        self.df['Increase_or_Decrease'] = self.df.apply(
            lambda row: 'Increase' if row['close'] > row['open'] else 'Decrease', axis=1
        )
        self.df['Price_Change'] = self.df['close'] - self.df['open']
        self.df['pct_change'] = ((self.df['close'] - self.df['open']) / self.df['open']) * 100
        self.df['volatility'] = (self.df['high'] - self.df['low']) / self.df['open']
        self.df['year'] = self.df['date'].dt.year
        self.df['month'] = self.df['date'].dt.month
        self.df['day'] = self.df['date'].dt.day

    def log_transform_and_standardize_prices(self):
        price_cols = ['open', 'high', 'low', 'close', 'adj_close']
        for col in price_cols:
            self.df[f'log_{col}'] = np.log(self.df[col])
        log_cols = [f'log_{col}' for col in price_cols]
        scaled_prices = self.scaler_price.fit_transform(self.df[log_cols])
        for i, col in enumerate(log_cols):
            self.df[f'z_{col}'] = scaled_prices[:, i]

    def transform_and_standardize_volume(self):
        self.df['log_volume'] = np.log(self.df['volume'])
        self.df['z_log_volume'] = self.scaler_volume.fit_transform(self.df[['log_volume']])

    def transform_and_standardize_volatility(self):
        self.df['volatility'] = self.df['volatility'].replace(0, 1e-6)
        self.df['log_volatility'] = np.log(self.df['volatility'])
        self.df['z_log_volatility'] = self.scaler_volatility.fit_transform(self.df[['log_volatility']])

    def add_outlier_flags(self):
        columns_to_flag = [
            'open', 'close', 'high', 'low', 'adj_close',
            'volume', 'volatility', 'Price_Change', 'pct_change'
        ]
        for col in columns_to_flag:
            Q1 = self.df[col].quantile(0.25)
            Q3 = self.df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower = Q1 - 1.5 * IQR
            upper = Q3 + 1.5 * IQR
            self.df[f'{col}_outlier'] = ((self.df[col] < lower) | (self.df[col] > upper)).astype(int)

    def add_time_series_features(self):
        self.df['prev_adj_close'] = self.df['adj_close'].shift(1)
        self.df['5_day_ma'] = self.df['adj_close'].rolling(window=5).mean()
        self.df['price_range'] = self.df['high'] - self.df['low']
        self.df['return'] = self.df['adj_close'].pct_change()
        self.df['return_1d'] = self.df['adj_close'].pct_change(1)
        self.df['return_3d'] = self.df['adj_close'].pct_change(3)
        self.df['volatility_3d'] = self.df['adj_close'].rolling(3).std()
        self.df['ma_3d'] = self.df['adj_close'].rolling(3).mean()
        self.df['ma_7d'] = self.df['adj_close'].rolling(7).mean()
        self.df['high_low_ratio'] = self.df['high'] / self.df['low']
        self.df = self.df.dropna()

    def run(self):
        self.convert_date()
        self.create_classification_and_features()
        self.log_transform_and_standardize_prices()
        self.transform_and_standardize_volume()
        self.transform_and_standardize_volatility()
        self.add_outlier_flags()
        self.add_time_series_features()
        return self.df

# Import pipeline from saved script (if needed)
# from walmart_pipeline import WalmartStockPipeline

# STEP 1: Load raw dataset into `df`
import kagglehub
import pandas as pd
import os

# Download latest version
path = kagglehub.dataset_download("abdmoiz/walmart-stock-data-2025")
print("Path to dataset files:", path)

# Find and load the CSV
for file in os.listdir(path):
    if file.endswith(".csv"):
        csv_path = os.path.join(path, file)
        break

df = pd.read_csv(csv_path)

# STEP 2: Run preprocessing on a new DataFrame (leave `df` untouched)
df_pipeline_input = df.copy()  # Copy manually here
pipeline = WalmartStockPipeline(df_pipeline_input)
df_processed = pipeline.run()

# STEP 3: (Optional) Preview or export
print(df_processed.head())

"""# Moving into Feature Selection"""

#set the target variables
df_processed['target_adj_close'] = df_processed['adj_close'].shift(-1)
df_processed = df_processed.dropna(subset=['target_adj_close'])

"""## Understanding Correlation"""

feature_cols = [
    'z_log_open', 'z_log_close', 'z_log_high', 'z_log_low', 'z_log_adj_close',
    'z_log_volume', 'z_log_volatility',
    'open_outlier', 'volume_outlier', 'volatility_outlier'
]

# Create a dataframe with only the selected features
corr_df = df_processed[feature_cols]
corr_df = df_processed[feature_cols]

# Compute correlation matrix
corr_matrix = corr_df.corr()

# Create a mask for the upper triangle
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))

# Set up the matplotlib figure
plt.figure(figsize=(12, 8))

# Draw the heatmap with the mask
sns.heatmap(corr_matrix,
            mask=mask,
            annot=True,
            fmt=".2f",
            cmap="coolwarm",
            center=0,
            square=True,
            linewidths=0.5)

plt.title("Triangle Correlation Heatmap with Target Variables")
plt.tight_layout()
plt.show()

"""## Running Further Tests"""

from sklearn.feature_selection import mutual_info_regression, RFE
from sklearn.inspection import permutation_importance
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
import pandas as pd
import numpy as np

# STEP 1: Define your input features and target
numeric_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()
exclude_cols = ['target_adj_close', 'target_direction']
feature_cols = [col for col in numeric_cols if col not in exclude_cols]

X = df_processed[feature_cols]
y = df_processed['target_adj_close']

# STEP 2: Mutual Information
mi_scores = mutual_info_regression(X, y, random_state=42)
mi_series = pd.Series(mi_scores, index=X.columns)

# STEP 3: RFE with Linear Regression (much faster)
rfe = RFE(LinearRegression(), n_features_to_select=5)
rfe.fit(X, y)
rfe_series = pd.Series(rfe.ranking_, index=X.columns)
rfe_series = rfe_series.map(lambda x: 1 if x == 1 else 0)

# STEP 4: Permutation Importance with RandomForest
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X, y)
perm_result = permutation_importance(model, X, y, n_repeats=10, random_state=42)
perm_series = pd.Series(perm_result.importances_mean, index=X.columns)

# STEP 5: Combine results into one table
result_df = pd.DataFrame({
    'Mutual_Info': mi_series,
    'RFE_Selected': rfe_series,
    'Permutation_Importance': perm_series
})

# Normalize for combined score
result_df['MI_Normalized'] = result_df['Mutual_Info'] / result_df['Mutual_Info'].max()
result_df['Permutation_Normalized'] = result_df['Permutation_Importance'] / result_df['Permutation_Importance'].max()
result_df['Score_Sum'] = result_df['MI_Normalized'] + result_df['RFE_Selected'] + result_df['Permutation_Normalized']
result_df = result_df.sort_values(by='Score_Sum', ascending=False)

# Display results
import seaborn as sns
import matplotlib.pyplot as plt

print(result_df[['Mutual_Info', 'RFE_Selected', 'Permutation_Importance', 'Score_Sum']].round(4))

# Visualize top 10
result_df['Score_Sum'].head(10).sort_values().plot(kind='barh', figsize=(8, 6), title='Top 10 Features by Combined Score')
plt.xlabel("Combined Score")
plt.tight_layout()
plt.show()

selected_features = [
    'ma_3d',
    'z_log_low',
    'z_log_high',
    'prev_adj_close',
    'log_adj_close',
    '5_day_ma',
    'high_low_ratio',
    'ma_7d'
]

"""# Testing for Multicollinearity"""

from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

# Use only numeric features (no categorical)
X_vif = df_processed[selected_features].copy()

# Add a constant column for intercept (required by VIF)
X_vif = add_constant(X_vif)

# Create a VIF dataframe
vif_data = pd.DataFrame()
vif_data["Feature"] = X_vif.columns
vif_data["VIF"] = [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]

print(vif_data.sort_values(by='VIF', ascending=False))

"""# Dropping some Columns due to Redundent Information"""

X_vif_clean = df_processed[selected_features].copy()
X_vif_clean = add_constant(X_vif_clean)

vif_data_clean = pd.DataFrame()
vif_data_clean["Feature"] = X_vif_clean.columns
vif_data_clean["VIF"] = [variance_inflation_factor(X_vif_clean.values, i) for i in range(X_vif_clean.shape[1])]
print(vif_data_clean.sort_values(by='VIF', ascending=False))

"""## Still showing multicollinearity, so I am dropping all redundent features"""

from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

# ✅ Define features to test for multicollinearity
vif_features = [
    'z_log_adj_close',
    'price_range',
    'volatility',
    'return',
    'ma_3d',
    'ma_7d',
    'high_low_ratio'
]

# ✅ Create feature matrix and add constant
X_vif = df_processed[vif_features].copy()
X_vif = add_constant(X_vif)

# ✅ Compute VIFs
vif_data = pd.DataFrame()
vif_data["Feature"] = X_vif.columns
vif_data["VIF"] = [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]

# ✅ Display
print(vif_data.sort_values(by="VIF", ascending=False))

selected_features = [
    'z_log_adj_close',
    'price_range',
    'volatility',
    'return',
    'ma_3d',
    'ma_7d',
    'high_low_ratio'
]

"""# Now I will automate the choosing of the regression models and hyperparameters"""

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.preprocessing import StandardScaler
import xgboost as xgb
import pandas as pd
import numpy as np

# STEP 1: Define features and target
X = df_processed[selected_features]
y = df_processed['target_adj_close']

# STEP 2: Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, test_size=0.2)

# STEP 3: Define model pipeline and hyperparameters
models = {
    'LinearRegression': (LinearRegression(), {}),
    'Ridge': (Ridge(), {'alpha': [0.01, 0.1, 1.0, 10]}),
    'Lasso': (Lasso(), {'alpha': [0.001, 0.01, 0.1, 1.0]}),
    'RandomForest': (RandomForestRegressor(), {
        'n_estimators': [100, 200],
        'max_depth': [None, 5, 10]
    }),
    'GradientBoosting': (GradientBoostingRegressor(), {
        'n_estimators': [100, 200],
        'learning_rate': [0.05, 0.1],
        'max_depth': [3, 5]
    }),
    'XGBoost': (xgb.XGBRegressor(objective='reg:squarederror'), {
        'n_estimators': [100, 200],
        'learning_rate': [0.05, 0.1],
        'max_depth': [3, 5]
    })
}

# STEP 4: Run GridSearch for each model
best_model = None
best_score = -np.inf
results = []

for name, (model, params) in models.items():
    print(f"Training {name}...")
    grid = GridSearchCV(model, params, scoring='r2', cv=5, n_jobs=-1)
    grid.fit(X_train, y_train)
    y_pred = grid.predict(X_test)
    score = r2_score(y_test, y_pred)

    results.append({
        'Model': name,
        'Best Params': grid.best_params_,
        'R2': score,
        'MAE': mean_absolute_error(y_test, y_pred),
        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred))
    })

    if score > best_score:
        best_score = score
        best_model = grid.best_estimator_

# STEP 5: Output model comparison
results_df = pd.DataFrame(results).sort_values(by='R2', ascending=False)
print(results_df)

# STEP 6: Use best_model to predict
y_pred_best = best_model.predict(X_test)

print(f"\nBest Model: {type(best_model).__name__}")
print(f"R² Score: {r2_score(y_test, y_pred_best):.4f}")

import joblib

# Save your best model
joblib.dump(best_model, 'best_stock_model.pkl')

# Optionally save the scaler if needed later
# joblib.dump(scaler, 'scaler.pkl')  # If you're using scaled input for prediction

"""# DISCLAIMER: This did not work out with great accuracy, but it has my deep learning code

# Using Deep Learning to Predict Classification
## I will predict if the price will increase or decrease
"""

# # 📦 Classification Pipeline to Predict Price Increase or Decrease

# from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV
# from sklearn.linear_model import LogisticRegression
# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
# import xgboost as xgb
# import pandas as pd

# # ✅ Define classification features (your engineered + selected ones)
# selected_features = [
#     'z_log_adj_close',
#     'price_range',
#     'volatility',
#     'return',
#     'ma_3d',
#     'ma_7d',
#     'high_low_ratio'
# ]

# # ✅ Define X and y
# df_processed = df_processed.copy()
# df_processed['target_direction'] = (df_processed['target_adj_close'] > df_processed['adj_close']).astype(int)

# X = df_processed[selected_features].copy()
# y = df_processed['target_direction'].copy()

# # ✅ Split train/test
# X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, test_size=0.2)

# # ✅ Setup stratified k-fold for balanced training
# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# # ✅ Define models + hyperparameter grids
# models = {
#     "LogisticRegression": (
#         LogisticRegression(max_iter=1000),
#         {}
#     ),
#     "RandomForest": (
#         RandomForestClassifier(),
#         {
#             "n_estimators": [100, 200],
#             "max_depth": [None, 5, 10]
#         }
#     ),
#     "GradientBoosting": (
#         GradientBoostingClassifier(),
#         {
#             "n_estimators": [100, 200],
#             "learning_rate": [0.05, 0.1],
#             "max_depth": [3, 5]
#         }
#     ),
#     "XGBoost": (
#         xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
#         {
#             "n_estimators": [100, 200],
#             "learning_rate": [0.05, 0.1],
#             "max_depth": [3, 5]
#         }
#     )
# }

# # ✅ Train, tune, and evaluate each model
# results = []

# for name, (model, param_grid) in models.items():
#     print(f"Training {name}...")
#     grid = RandomizedSearchCV(
#         estimator=model,
#         param_distributions=param_grid,
#         scoring='f1',
#         cv=skf,
#         n_iter=15,
#         n_jobs=-1,
#         random_state=42
#     )
#     grid.fit(X_train, y_train)
#     best_model = grid.best_estimator_
#     y_pred = best_model.predict(X_test)

#     results.append({
#         "Model": name,
#         "Best Params": grid.best_params_,
#         "Accuracy": accuracy_score(y_test, y_pred),
#         "Precision": precision_score(y_test, y_pred),
#         "Recall": recall_score(y_test, y_pred),
#         "F1 Score": f1_score(y_test, y_pred)
#     })

# # ✅ Output results
# results_df = pd.DataFrame(results).sort_values(by="F1 Score", ascending=False)
# print("\nFinal Model Results:")
# print(results_df)

# # ✅ Show classification report for best model
# best_model_name = results_df.iloc[0]['Model']
# final_model = models[best_model_name][0].set_params(**results_df.iloc[0]['Best Params'])
# final_model.fit(X_train, y_train)
# y_pred_final = final_model.predict(X_test)
# print(f"\nClassification Report for Best Model ({best_model_name}):")
# print(classification_report(y_test, y_pred_final))

# !pip install tqdm

# import torch
# import torch.nn as nn
# import torch.optim as optim
# from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler
# from sklearn.metrics import accuracy_score
# from sklearn.model_selection import train_test_split
# from tqdm.notebook import tqdm
# import matplotlib.pyplot as plt
# import pandas as pd
# import numpy as np

# # Split off a validation set
# X_train_sub, X_val, y_train_sub, y_val = train_test_split(
#     X_train, y_train, test_size=0.15, stratify=y_train, random_state=42)

# # Convert to tensors
# X_train_tensor = torch.tensor(X_train_sub.to_numpy(), dtype=torch.float32)
# y_train_tensor = torch.tensor(y_train_sub.to_numpy(), dtype=torch.float32).view(-1, 1)
# X_val_tensor = torch.tensor(X_val.to_numpy(), dtype=torch.float32)
# y_val_tensor = torch.tensor(y_val.to_numpy(), dtype=torch.float32).view(-1, 1)
# X_test_tensor = torch.tensor(X_test.to_numpy(), dtype=torch.float32)
# y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.float32).view(-1, 1)

# # Compute weights for WeightedRandomSampler
# class_counts = torch.bincount(y_train_tensor.view(-1).long())
# class_weights = 1. / class_counts.float()
# sample_weights = class_weights[y_train_tensor.view(-1).long()]
# sampler = WeightedRandomSampler(sample_weights, len(sample_weights))

# train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
# train_loader = DataLoader(train_dataset, batch_size=32, sampler=sampler)

# # BinaryClassifier with more layers
# class BinaryClassifier(nn.Module):
#     def __init__(self, input_dim):
#         super(BinaryClassifier, self).__init__()
#         self.network = nn.Sequential(
#             nn.Linear(input_dim, 128),
#             nn.BatchNorm1d(128),
#             nn.ReLU(),
#             nn.Dropout(0.3),
#             nn.Linear(128, 64),
#             nn.BatchNorm1d(64),
#             nn.ReLU(),
#             nn.Dropout(0.3),
#             nn.Linear(64, 32),
#             nn.BatchNorm1d(32),
#             nn.ReLU(),
#             nn.Dropout(0.2),
#             nn.Linear(32, 16),
#             nn.ReLU(),
#             nn.Linear(16, 1)
#         )

#     def forward(self, x):
#         return self.network(x)

# # Focal Loss implementation
# class FocalLoss(nn.Module):
#     def __init__(self, alpha=1, gamma=2, logits=True, reduce=True):
#         super(FocalLoss, self).__init__()
#         self.alpha = alpha
#         self.gamma = gamma
#         self.logits = logits
#         self.reduce = reduce

#     def forward(self, inputs, targets):
#         if self.logits:
#             BCE_loss = nn.BCEWithLogitsLoss(reduction='none')(inputs, targets)
#         else:
#             BCE_loss = nn.BCELoss(reduction='none')(inputs, targets)
#         pt = torch.exp(-BCE_loss)
#         F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss

#         return F_loss.mean() if self.reduce else F_loss

# # Train and evaluate
# model = BinaryClassifier(input_dim=X_train.shape[1])
# optimizer = optim.Adam(model.parameters(), lr=0.001)
# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)
# criterion = FocalLoss()

# best_val_loss = float('inf')
# best_model_state = None
# loss_history = []

# for epoch in range(30):
#     model.train()
#     total_loss = 0
#     for inputs, labels in train_loader:
#         optimizer.zero_grad()
#         outputs = model(inputs)
#         loss = criterion(outputs, labels)
#         loss.backward()
#         optimizer.step()
#         total_loss += loss.item()
#     avg_loss = total_loss / len(train_loader)
#     loss_history.append(avg_loss)

#     # Validate
#     model.eval()
#     with torch.no_grad():
#         val_outputs = model(X_val_tensor)
#         val_loss = criterion(val_outputs, y_val_tensor)
#         scheduler.step(val_loss)

#         if val_loss < best_val_loss:
#             best_val_loss = val_loss
#             best_model_state = model.state_dict()

#     print(f"Epoch {epoch+1}/30 - Train Loss: {avg_loss:.4f} - Val Loss: {val_loss:.4f}")

# # Load best model
# model.load_state_dict(best_model_state)

# # Evaluate on test
# model.eval()
# with torch.no_grad():
#     probs = torch.sigmoid(model(X_test_tensor))
#     y_pred_label = (probs > 0.5).int()
#     acc = accuracy_score(y_test_tensor, y_pred_label)
#     print(f"\n✅ Final Test Accuracy (best model): {acc:.4f}")